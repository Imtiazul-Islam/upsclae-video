{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install Gradio for the UI and the latest llama-cpp-python\n!pip install llama-cpp-python huggingface-hub gradio","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\n\n# Downloading the Lexi-Llama-3.1-8B GGUF\nmodel_path = hf_hub_download(\n    repo_id=\"bartowski/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF\",\n    filename=\"Llama-3.1-8B-Lexi-Uncensored-V2-Q5_K_M.gguf\"\n)\n\nprint(f\"Model ready at: {model_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom llama_cpp import Llama\n\n# Optimization settings for Kaggle CPU\n# n_threads should usually match the number of physical cores. Kaggle gives 2-4.\n# n_ctx: 4096 is usually enough for chat; 8192 might slow down CPU inference over time.\nllm = Llama(\n    model_path=model_path,\n    n_ctx=8192,           \n    n_threads=4,          \n    n_batch=512,         # Number of tokens to process in parallel during prompt eval\n    chat_format=\"llama-3\",\n    verbose=False\n)\n\nprint(\"LLM Engine Initialized.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\n\n# Define the Character Persona\n# Since you play Genshin and HSR, I've refined the Zhongli persona\nCHARACTER_NAME = \"Your_Character_Name\"\nSYSTEM_PROMPT = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nYou are {CHARACTER_NAME} from PLACE. You are INFO.\nStyle: .\nTraits: .\nRule: Stay in character at all times. Do not mention you are an AI.<|eot_id|>\"\"\"\n\ndef predict(message, history):\n    # Convert Gradio history format to Llama-3 Chat format\n    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n    \n    for val in history:\n        if val[0]: messages.append({\"role\": \"user\", \"content\": val[0]})\n        if val[1]: messages.append({\"role\": \"assistant\", \"content\": val[1]})\n    \n    messages.append({\"role\": \"user\", \"content\": message})\n\n    # Generate streaming response\n    response = \"\"\n    completion = llm.create_chat_completion(\n        messages=messages,\n        stream=True,\n        temperature=0.7, # Slightly lower for more consistent personality\n        top_p=0.9,\n        max_tokens=512\n    )\n\n    for chunk in completion:\n        delta = chunk['choices'][0]['delta']\n        if 'content' in delta:\n            token = delta['content']\n            response += token\n            yield response\n\n# Create a clean, dark-themed UI\nwith gr.Blocks(theme=gr.themes.Soft()) as demo:\n    gr.Markdown(f\"# üèõÔ∏è {CHARACTER_NAME} Chat\")\n    gr.Markdown(f\"Experience a conversation with the Consultant of Wangsheng Funeral Parlor.\")\n    \n    chatbot = gr.ChatInterface(\n        fn=predict,\n        examples=[\"How Are You?\", \"What Can You Do?\", \"etc\"],\n        cache_examples=False,\n    )\n\ndemo.queue().launch(share=True) # share=True creates a public link you can open on your phone","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}